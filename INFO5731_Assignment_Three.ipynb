{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVCZ2UI-Yr5o"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import requests\n",
        "import collections\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def addToArray(values,reference):\n",
        "  for val in values:\n",
        "    reference.append(val.text)\n",
        "link=[\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "rating=[]\n",
        "text=[]\n",
        "title=[]\n",
        "for val in link:\n",
        "  paper=requests.get(val,headers={'User-Agent':'Chrome/85.0.4183.121'})\n",
        "  s = BeautifulSoup(paper.content, 'html.parser')\n",
        "  addToArray(s.find_all('div', class_='text show-more__control'),text)\n",
        "  addToArray(s.find_all('a',class_='title'),title)\n",
        "  addToArray(s.find_all(name='span',class_='rating-other-user-rating'),rating)\n",
        "imdbDataFrame=pd.DataFrame({'comment':text,'title':title})\n",
        "imdbDataFrame.to_csv('imdb.csv')\n",
        "\n",
        "def freqNgram(val,nB):\n",
        "  dArray=[]\n",
        "  for i in val:\n",
        "    for j in  createNgrams(i, nB):\n",
        "      dArray.append(j)\n",
        "  freq=collections.Counter(dArray)\n",
        "  return freq\n",
        "threeGram=freqNgram(imdbDataFrame['comment'],3)\n",
        "problem={}\n",
        "oneFrame=freqNgram(imdbDataFrame['comment'],1)\n",
        "twoFrame=freqNgram(imdbDataFrame['comment'],2)\n",
        "\n",
        "def  createNgrams(val, nA):\n",
        "  val = val.lower()\n",
        "  val = re.sub(r'[^a-zA-Z0-9\\s]', ' ', val)\n",
        "  tokens = [token for token in val.split(\" \") if token != \"\"]\n",
        "  output = list(ngrams(tokens, nA))\n",
        "  return output\n",
        "\n",
        "for val in twoFrame.keys():\n",
        "  problem[val]=twoFrame[val]/oneFrame[eval(\"'\"+val[0]+\"',\")]\n",
        "\n",
        "nounPhrase=[]\n",
        "n_l_p = spacy.load(\"en_core_web_sm\")\n",
        "for val in imdbDataFrame['comment']:\n",
        "  dox = n_l_p(val)\n",
        "  for chunk in dox.noun_chunks:\n",
        "    nounPhrase.append(chunk)\n",
        "nounFreq=collections.Counter(nounPhrase)\n",
        "\n",
        "nounPhrase[1:15]\n",
        "\n",
        "big=max(nounFreq.values())\n",
        "frequencyRelative=[]\n",
        "for val in nounFreq.values():\n",
        "  var=val/big\n",
        "  frequencyRelative.append(var)\n",
        "d={'noun_phrases':noun_phrases,'frequencyRelative':frequencyRelative}\n",
        "frequencyRelative_df=pd.DataFrame(data=d)\n",
        "frequencyRelative_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "def TDM(dox, IDs=None):\n",
        "    vect = CountVectorizer(lowercase=True, stop_words=None)\n",
        "    tdm = vect.fit_transform(dox)\n",
        "    tdm_feature_names = vect.get_feature_names()\n",
        "    #\n",
        "    dataFrame = pd.DataFrame(tdm.toarray(), columns=tdm_feature_names, dtype=\"float64\")\n",
        "    if IDs is not None:\n",
        "        dataFrame.index = IDs    \n",
        "    return dataFrame\n",
        "TDM(imdbDataFrame['comment'])\n",
        "\n",
        "\n",
        "stopWords = set(stopwords.words('english')) \n",
        "class TokenLemma:\n",
        "    disregard = [':', '\"', '``', \"''\", '`',',','.',';']\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.disregard]\n",
        "findTerm = 'movie'\n",
        "tokenizer=TokenLemma()\n",
        "token_stop = tokenizer(' '.join(stopWords))\n",
        "documents = imdbDataFrame['comment']\n",
        "v = TfidfVectorizer(stop_words=token_stop, \n",
        "                              tokenizer=tokenizer)\n",
        "doc_vectors = v.fit_transform([findTerm] + documents)\n",
        "cosSim = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
        "docScores = [item.item() for item in cosSim[1:]]\n",
        "\n",
        "cosSim\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: \n",
        "https://github.com/shivanigarlapati12/INFO-5731"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}