{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In-class-exercise-05.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJf4lJkgVZE"
      },
      "source": [
        "## The fifth In-class-exercise (9/30/2020, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLesiXRlgVZH"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkdQWIIlgVZJ",
        "outputId": "8e821be2-c2fc-48e0-dbe7-e3a5ec8f334d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Please write your code here\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,338 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,673 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,110 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,100 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [856 kB]\n",
            "Fetched 8,349 kB in 4s (2,149 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 79.2 MB of archives.\n",
            "After this operation, 268 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 85.0.4183.121-0ubuntu0.18.04.1 [1,117 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 85.0.4183.121-0ubuntu0.18.04.1 [70.3 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 85.0.4183.121-0ubuntu0.18.04.1 [3,432 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 85.0.4183.121-0ubuntu0.18.04.1 [4,415 kB]\n",
            "Fetched 79.2 MB in 5s (15.4 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144618 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_85.0.4183.121-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_85.0.4183.121-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_85.0.4183.121-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_85.0.4183.121-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (85.0.4183.121-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blQm9o1xgtMv"
      },
      "source": [
        "import urllib.request\n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "op = webdriver.ChromeOptions()\n",
        "op.add_argument('-headless')\n",
        "op.add_argument('-no-sandbox')\n",
        "op.add_argument('-disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=op)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4wuzfrKgybm"
      },
      "source": [
        "\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nZU-VgIg1oi"
      },
      "source": [
        "def search_data(driver, class_name_val, css_selector_val):\n",
        "  ans = list()\n",
        "  values = driver.find_elements_by_class_name(class_name_val)\n",
        "  for value in values:\n",
        "    ans.append(value.find_element_by_css_selector(css_selector_val).text)\n",
        "  return ans\n",
        "  pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7jnY4upg5Hq",
        "outputId": "f1d97025-20fc-4cdf-9c51-43bf15cf4c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "import time\n",
        "driver.get(\"https://www.imdb.com/title/tt7286456/reviews?sort=userRating&dir=desc&ratingFilter=0\")\n",
        "x=0\n",
        "while(x<20):\n",
        "  button = driver.find_element_by_xpath('//*[@id =\"load-more-trigger\"]').click()\n",
        "  x +=1\n",
        "  time.sleep(7)\n",
        "\n",
        "reviews = fetch_data(driver, \"content\", \".text.show-more__control\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9c1a3ff36251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".text.show-more__control\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiDQjV7Cg9C8"
      },
      "source": [
        "required_df = pd.DataFrame(reviews,columns = ['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIuBIPW6g_ND"
      },
      "source": [
        "\n",
        "required_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKwnlz42hCVU",
        "outputId": "7041271e-bc91-47f2-dc37-0e870983f808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "wordsList = set(stopwords.words('english'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW10xtK8hICS",
        "outputId": "a927c5e0-bd79-404e-f91a-0e6b9171eafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "freqWords = list(pd.Series(' '.join(required_df['data']).split()).value_counts()[:15].index)\n",
        "#top 15 rare words\n",
        "rareWords = list(pd.Series(' '.join(required_df['data']).split()).value_counts()[-15:].index)\n",
        "\n",
        "!pip install textblob\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "required_df[\"punc\"] = required_df['data'].apply(lambda x:''.join([i for i in x if i not in string.punctuation])).to_frame()\n",
        "required_df[\"lcase\"] = required_df['punc'].apply(lambda x: x.lower()).to_frame()\n",
        "required_df[\"stopwords\"] = required_df['lcase'].apply(lambda x:' '.join([i for i in x.split() if i not in list_of_words])).to_frame()\n",
        "required_df[\"freqWords\"] = required_df['stopwords'].apply(lambda x: ' '.join([i for i in x.split() if i not in frequent_words])).to_frame()\n",
        "required_df[\"rareWords\"] = required_df['freqWords'].apply(lambda x: ' '.join([i for i in x.split() if i not in rare_words])).to_frame()\n",
        "required_df[\"spellCheck\"]=required_df['rareWords'].apply(lambda x: str(TextBlob(x).correct())).to_frame()\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "required_df[\"tokenization\"] = required_df['spellCheck'].apply(lambda x: word_tokenize(str(x))).to_frame()\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "import textblob\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-baaa0bb81678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfreqWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#top 15 rare words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrareWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'required_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUY62BXmhpJE",
        "outputId": "cecb1bb1-5505-431a-cbc1-c5c331394775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "required_df[\"stemming\"] = required_df['tokenization'].apply(lambda x: ' '.join([ps.stem(i) for i in x])).to_frame()\n",
        "required_df[\"final_data\"] = required_df['tokenization'].apply(lambda x: ' '.join([textblob.Word(i).lemmatize() for i in x])).to_frame()\n",
        "\n",
        "required_df\n",
        "\n",
        "required_df1 = pd.DataFrame(required_df[[\"final_data\"]])\n",
        "required_df1\n",
        "def calculateTF(line):\n",
        "  words = line.split(\" \")\n",
        "  values = len(set(words))\n",
        "  return words.count(words[0])/values\n",
        "  pass\n",
        "  import math \n",
        "\n",
        "def calculateIDF(line):\n",
        "  words = sentence.lower().split(\" \")\n",
        "  res = words.count(words[0])\n",
        "  return math.log(len(words)/res, 10)\n",
        "  pass\n",
        "required_df1[\"word used for TF-IDF\"] = required_df1[\"final_data\"].apply(lambda x : x.split(\" \")[0])\n",
        "required_df1[\"tf\"] = required_df1[\"final_data\"].apply(lambda x : round(calculateTF(x),2))\n",
        "required_df1[\"idf\"] = required_df1[\"final_data\"].apply(lambda x : round(calculateIDF(x),2))\n",
        "required_df1[\"tf-idf\"] = required_df1[\"tf\"]*required_df1[\"idf\"]\n",
        "required_df1\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "lemmatized = required_df1[\"final_data\"].values.tolist()\n",
        "pos=[]\n",
        "count=0\n",
        "for i in lemmatized:\n",
        "  text = word_tokenize(i)\n",
        "  count=count+1\n",
        "  pos.append(nltk.pos_tag(text))\n",
        "  from collections import Counter\n",
        "\n",
        "def get_pos(value,pos):\n",
        "  pos_counts=[]\n",
        "  for j in pos:\n",
        "    counts = Counter(tag for word,tag in j)  \n",
        "    if(counts.get(value)!=None):\n",
        "      pos_counts.append(counts.get(value))\n",
        "    else:\n",
        "      pos_counts.append(0)\n",
        "  return pos_counts\n",
        "  required_df1[\"count of nouns\"] = pd.DataFrame(get_pos(\"NN\",pos))\n",
        "required_df1[\"count of adverb\"] = pd.DataFrame(get_pos(\"RB\",pos))\n",
        "required_df1[\"count of cardinal digits\"] = pd.DataFrame(get_pos(\"CD\", pos))\n",
        "required_df1[\"count of Verbs\"] = pd.DataFrame(get_pos(\"VB\",pos))\n",
        "required_df1[\"count of Determiners\"] = pd.DataFrame(get_pos(\"DT\",pos)) \n",
        "required_df1[\"count of adjectives\"] = pd.DataFrame(get_pos(\"JJ\",pos)) \n",
        "required_df1[\"count of interjenctions\"] = pd.DataFrame(get_pos(\"UH\",pos))\n",
        "required_df1[\"count of Prepositions\"] = pd.DataFrame(get_pos(\"IN\",pos))\n",
        "first = pd.DataFrame(get_pos(\"PRP\",pos))\n",
        "second = pd.DataFrame(get_pos(\"PRP$\",pos))\n",
        "third = pd.DataFrame(get_pos(\"WP\",pos))\n",
        "fourth = pd.DataFrame(get_pos(\"WP$\",pos))\n",
        "\n",
        "required_df1[\"count of pronouns\"] = first[0]+second[0]+third[0]+fourth[0]\n",
        "first = required_df1[\"final_data\"].apply(lambda x : len(x.split(\" \")))\n",
        "required_df1[\"Density of pronouns\"] = required_df1[\"count of pronouns\"]/first[0]\n",
        "required_df1[\"Density of nouns\"] = required_df1[\"count of nouns\"]/first[0]\n",
        "required_df1[\"count of conjunctions\"] = pd.DataFrame(get_pos(\"CC\",pos))\n",
        "required_df1[\"Density of prepositions\"] = required_df1[\"count of Prepositions\"]/first[0]\n",
        "required_df1[\"Density of conjunctions\"] = required_df1[\"count of conjunctions\"]/first[0]\n",
        "\n",
        "required_df1\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import Tree\n",
        "import en_core_web_sm\n",
        "en_nlp = spacy.load(\"en\")\n",
        "\n",
        "def Dependency_trees(tree_node):\n",
        "    if 0<tree_node.n_lefts + tree_node.n_rights:\n",
        "        return Tree(tree_node.orth_, [Dependency_trees(child) for child in tree_node.children])\n",
        "    else:\n",
        "        return tree_node.orth_\n",
        "for temp in required_df1['final_data']:\n",
        "        doc = en_nlp(temp)\n",
        "        try:\n",
        "          for sent in doc.sents:\n",
        "            Dependency_trees(sent.root).pretty_print()\n",
        "        except:\n",
        "          continue\n",
        "pd.set_option('display.max_rows', None)\n",
        "required_df1\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8c25d2ee4fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrequired_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stemming\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrequired_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"final_data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtextblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    }
  ]
}