{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In_class_exercise_07.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WELR3uS5zuAh"
      },
      "source": [
        "# **The seventh in-class-exercise (20 points in total, 10/21/2020)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoNXbnC_zuAn"
      },
      "source": [
        "Question description: In the last in-class-exercise (exercise-06), you collected the titles of 100 articles about data science, natural language processing, and machine learning. The 100 article titles will be used as the text corpus of this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ZslqwizuAr"
      },
      "source": [
        "## (1) (8 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8l2PQEnzuAu",
        "outputId": "880cdc26-4ffe-400e-b6d1-be7a6f6c2095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Write your code here\n",
        "import nltk; \n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMGYeJa2z5pq",
        "outputId": "05619ce3-89ca-4608-9ec3-08b362541709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "!pip install spacy\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.2)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=a635cc9148b33440b4029fe3e607112676dc74e29192cbd1638114be99ba6acb\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.15 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1N6S7RGz7c-"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2W3Iow50NVu"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2iN9L_v1zIS"
      },
      "source": [
        "import pandas as pd\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19HxHkIl3eCu",
        "outputId": "29981fca-eeb9-45af-95a7-df3279800a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def insert_into_array(values,ref_array):\n",
        "  for i in values:\n",
        "    ref_array.append(i.text)\n",
        "titles=[]\n",
        "for i in range(0,100,10):\n",
        "  URL=\"http://citeseerx.ist.psu.edu/search?q=data+science&t=doc&sort=rlv&start=\"+str(i)\n",
        "  page=requests.get(URL,headers={'User-Agent':'Chrome/85.0.4183.121'})\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  insert_into_array(soup.find_all('a', class_='remove doc_details'),titles)\n",
        "print(titles)\n",
        "df = pd.DataFrame (titles,columns=['titles'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\nData Streams: Algorithms and Applications\\n                  ', '\\n                  Bayesian Data Analysis\\n                  ', '\\n                  Voronoi diagrams -- a survey of a fundamental geometric data structure\\n                  ', '\\n                  From Data Mining to Knowledge Discovery in Databases.\\n                  ', '\\n                  Survey of clustering algorithms\\n                  ', '\\n                  Thresholding of statistical maps in functional neuroimaging using the false discovery rate.\\n                  ', '\\n                  Maintaining knowledge about temporal intervals\\n                  ', '\\n                  Status quo bias in decision making\\n                  ', '\\n                  Rough Sets.\\n                  ', '\\n                  Modeling TCP Throughput: A Simple Model and its Empirical Validation\\n                  ', '\\n                  Compressive sampling\\n                  ', '\\n                  Search and replication in unstructured peer-to-peer networks\\n                  ', '\\n                  Dynamic Bayesian Networks: Representation, Inference and Learning\\n                  ', '\\n                  Motivational and self-regulated learning components of classroom academic performance\\n                  ', '\\n                  Probabilistic Inference Using Markov Chain Monte Carlo Methods\\n                  ', '\\n                  A theory of social comparison processes,”\\n                  ', '\\n                  Group formation in large social networks: membership, growth, and evolution\\n                  ', '\\n                  Illusion and well-being: A social psychological perspective on mental health.\\n                  ', '\\n                  Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation\\n                  ', '\\n                  Natural Selection\\n                  ', '\\n                  Foreign Direct Investment and Relative Wages: Evidence from Mexico’s Maquiladoras\\n                  ', '\\n                  Indexing and Querying XML Data for Regular Path Expressions\\n                  ', '\\n                  A survey of data provenance in e-science\\n', '\\n                  A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions\\n                  ', '\\n                  The earth is round (p < .05\\n                  ', '\\n                  Citizens as sensors: the world of volunteered geography.”\\n                  ', '\\n                   Mechanism Design for Data Science\\n', '\\n                  Foundations of data science\\n', '\\n                  Foundations of data science\\n', '\\nData Sciences UK Ltd.\\n                  ', '\\n                  EMI Music Data Science\\n', '\\nData Sciences Laboratory\\n                  ', '\\n                  INTRODUCTION TO DATA SCIENCE\\n', '\\n                  Database Mining: A Performance Perspective\\n                  ', '\\n                  Functional Data Structures\\n                  ', '\\n                  Towards Cognitive Automation of Data Science\\n', '\\n                  My Solution to the EMC Data Science Global\\n                  ', '\\n                  MOOCdb: Developing Data Standards for MOOC Data Science\\n', '\\n                  A survey of outlier detection methodologies\\n                  ', '\\n                  Gender, Agricultural Productivity and the Theory of the Household\\n                  ', '\\n                  SPARSKIT: a basic tool kit for sparse matrix computations - Version 2\\n                  ', '\\n                  On Clustering Validation Techniques\\n                  ', '\\n                  Chief. Data Sciences Division1\\n                  ', '\\n                  Industrial development in cities\\n                  ', '\\n                  Parallel Computing for Data Science with Examples in R and Beyond\\n                  ', '\\nData Science in the Statistics Curricula: Preparing Students to \"Think with Data\"\\n                  ', '\\n                  Guest Editorial EDITOR’S COMMENTS The Business of Business Data Science in IS Journals\\n                  ', '\\n                  Reconstructing past climate from noisy data. Science 306: 679–682\\n                  ', '\\n                  A Critique of Software Defect Prediction Models\\n                  ', '\\n                  A new approach to abstract syntax with variable binding\\n                  ', '\\n                  Stochastic global optimization\\n                  ', '\\n                  Fidelity and yield in a volcano monitoring sensor network\\n                  ', '\\n                  The Netflix Prize\\n                  ', '\\n                  A rational analysis of the selection task as optimal data selection\\n                  ', '\\n                  A Tutorial on (Co)Algebras and (Co)Induction\\n                  ', '\\n                  Statistical Analysis of a Telephone Call Center: A Queueing Science Perspective\\n                  ', '\\n                  1 Data Science and Mass Media: Seeking a Hermeneutic Ethics of Information\\n                  ', '\\n                  Editorial—Big Data, Data Science, and Analytics: The Opportunity and Challenge for Is Research\\n                  ', '\\n                  Deep Feature Synthesis: Towards Automating Data Science Endeavors\\n                  ', '\\nData Science and Distributed Intelligence: Recent Developments and Future Insights\\n                  ', '\\nDATA SCIENCE AND ITS RELATIONSHIP TO BIG DATA AND DATA-DRIVEN DECISION MAKING ORIGINAL ARTICLE\\n                  ', '\\n                  •\\u202f Organic data science 4. Closing thoughts\\n                  ', '\\n                  e-Infrastructure Requirements for Big Data Science\\n', '\\n                  The Electronic Universe: Network Delivery of Data, Science and Discovery\\n                  ', '\\n                  Editorial: special issue on learning from imbalanced data sets\\n                  ', '\\n                  ICML 2014 AutoML Workshop Cognitive Automation of Data Science\\n', '\\n                  Matrix Completion with Noise\\n                  ', '\\n                  Qualitative Case Study Methodology: Study Design and Implementation for Novice Researchers\\n                  ', '\\n                  The Investment behavior and performance of various investor\\n                  ', '\\n                  E-commerce: The role of familiarity and trust\\n                  ', '\\n                  A Software Interface for Supporting the Application of Data Science to Optimisation\\n                  ', '\\n                  Technology Probes: Inspiring Design for and with Families\\n                  ', '\\n                  TEACHING PRECURSORS TO DATA SCIENCE IN INTRODUCTORY AND SECOND COURSES IN STATISTICS\\n                  ', '\\nData Science of the People, for the People, by the People: A Viewpoint on an Emerging Dichotomy\\n                  ', '\\nData Science Study Protocols for Investigating Lifetime and Degradation of PV Technology Systems\\n                  ', '\\n                  Maps of random walks on complex networks reveal community structure\\n                  ', '\\n                  A Macroscope in the Redwoods\\n                  ', '\\n                  Thrust Area in Data Science-Big Data and Data Analytics\\n                  ', '\\n                  With their book XML and Web Technologies for Data Sciences with R, Deborah Nolan and\\n                  ', '\\nData Management and Transfer in High-Performance Computational Grid Environments\\n                  ', '\\n                  A new approach to decoding life: systems biology\\n                  ', '\\n                  Geometry for the selfish herd.\\n                  ', '\\n                  PERSPECTIVE Let&aposs Make Gender Diversity in Data Science a Priority Right from the Start\\n                  ', '\\nDataHub: Collaborative Data Science & Dataset Version Management at Scale\\n                  ', '\\n                  To be published in Data Science Journal, 2005 Digital Object Identifiers for scientific data\\n', '\\n                  OkCupid Data for Introductory Statistics and Data Science Courses\\n                  ', '\\n                  The Role of Income Aspirations in Individual Happiness\\n                  ', '\\n                  Predicting tie strength with social media\\n                  ', '\\n                  Intel “Big Data ” Science and Technology Center Vision and Execution Plan\\n                  ', '\\n                  Teaching Parallelism Without Programming: A Data Science Curriculum for Non-CS Students\\n                  ', '\\n                   How Computational Statistics Became the Backbone of Modern Data Science\\n', '\\n                  Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces\\n                  ', '\\n                  Model selection in ecology and evolution.\\n                  ', '\\n                  Local Ground:\\xa0 A\\xa0Toolkit\\xa0Supporting\\xa0Metarepresentational Competence in\\xa0Data\\xa0Science\\n', '\\n                  Visual Exploration of Formal Requirements for Data Science Demand Analysis\\n                  ', '\\n                  Earth Data, Science Writing, and Peer Review page 1 Earth Data, Science Writing, and Peer Review in a Large General Education Oceanography Class\\n                  ', '\\n                  An Online Algorithm for Segmenting Time Series\\n                  ', '\\n                  OUTLINE AND EXERCISES FOR A NOVEL INTRODUCTORY COURSE IN DATA SCIENCE AND VISUALIZATION\\n                  ', '\\n                   Computational Data Sciences for Actionable Insights on Climate Extremes and Uncertainty\\n                  ', '\\n                  The New Computational and Data Sciences Undergraduate Program at George Mason University 1 Data-Intensive Science: A New Vision for Science Education\\n                  ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6nfPAMX3mEF"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyI98y6i3s7o",
        "outputId": "aa0788ec-1a0b-4d6c-8a07-88ecff892fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(titles))\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "print(data_words[:1])\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['data', 'streams', 'algorithms', 'and', 'applications']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UiLPtsc373h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4xJeF8jzuA6"
      },
      "source": [
        "## (2) (8 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSR7s-lm3j9G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYAgYxbYzuA9"
      },
      "source": [
        "# Write your code here\n",
        "number_of_topics = 7\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udFoh-R34RwD"
      },
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model \n",
        "    return dictionary,doc_term_matrix"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9UlBhdh4VNy"
      },
      "source": [
        "from gensim.models import LsiModel\n",
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    \n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iclMy6Eq4ZGG"
      },
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start, step):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSM0rLEp5TW6"
      },
      "source": [
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix\n",
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel\n",
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "      model = LsiModel(doc_term_matrix, num_topics, id2word = dictionary)  # train model\n",
        "      model_list.append(model)\n",
        "            coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "      coherence_values.append(coherencemodel.get_coherence())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACO7ce8e5PIR"
      },
      "source": [
        "def plot_graph(doc_clean,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD1hGufq5wnO"
      },
      "source": [
        "start,stop,step=2,12,1\n",
        "clean_text=preprocess_data(titles)\n",
        "plot_graph(clean_text,start,stop,step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFev9ErW5y9v"
      },
      "source": [
        "number_of_topics=2\n",
        "words=10\n",
        "clean_text=preprocess_data(titles)\n",
        "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXJLsUSpzuBJ"
      },
      "source": [
        "## (3) (4 points) Compare the results generated by the two topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAf6pql5sTJ"
      },
      "source": [
        "LSA is utilized for recognizing topics in information base of text. The center thought is to change the information over to lattice\n",
        " which contains the terms and the themes. In light of the matric we can successfully perform Topic displaying by disintegrating the\n",
        "  terms to a seperate point term grid and archive term lattice. We acheived a choherance score around .45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q87yt_Yl6BmD"
      },
      "source": [
        "LDA is utilized for tackling theme demonstrating issues. The center thought is to deal with the record term network and the point term grid\n",
        " and perfom the subject displaying. LDA utilizes the preparation informational collection and gains from the Unstructured gatherings. \n",
        " So a large portion of the themes are very comparable. We acheived intelligibility score around .73"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t057djiA6Sod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78TeFD5_zuBL"
      },
      "source": [
        "# Write your answer here (no code needed for this question)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}