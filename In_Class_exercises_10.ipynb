{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "In_Class_exercises-10.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU4mXXO9TC21"
      },
      "source": [
        "# In class exercise 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyRiIFXSTC23"
      },
      "source": [
        "The purpose of the exercise is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K means, \n",
        "DBSCAN,\n",
        "Hierarchical clustering. \n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4jGmVoUTWx3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATkJsPmzTgDK"
      },
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "df=pd.read_csv(\"C:\\Users\\shiva\\Downloads\\archive (2).zip\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df = df[df['Reviews'].notnull()]\n",
        "stop = stopwords.words('english')\n",
        "df['after_punct_less'] = df['Reviews'].str.replace('[^\\w\\s].#','')\n",
        "df['after_rm_stopwords'] =df['after_punct_less'].apply(lambda a: \" \".join(a for a in a.split() if a not in stop))\n",
        "df['after_numerics']=df['after_rm_stopwords'].str.replace('[0-9]','')\n",
        "df['after_lowercasing'] =df['after_numerics'].apply(lambda a: \" \".join(a.lower() for a in a.split()))\n",
        "\n",
        "df['after_stemming']=df['after_lowercasing'].apply(lambda a: \" \".join([st.stem(word) for word in a.split()]))\n",
        "df['cleaned_text'] = df['after_stemming'].apply(lambda a: \" \".join([Word(word).lemmatize() for word in a.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUu71rOiTC25"
      },
      "source": [
        "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yis7YdvDVXWc"
      },
      "source": [
        "df['cleaned_text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s84ElwoVgOT"
      },
      "source": [
        "df_1=(df.sample(n=4000)).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KTEXEOrXSAf"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf = tfidf_vect.fit_transform(df_1['cleaned_text'].values)\n",
        "tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQfVVvrTXVe3"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model_tf = KMeans(n_clusters = 4, n_jobs = -1,random_state=89)\n",
        "model_tf.fit(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IzhpVaDXc9Q"
      },
      "source": [
        "labels_tf = model_tf.labels_\n",
        "cluster_center_tf=model_tf.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YgMILmXf-F"
      },
      "source": [
        "terms1 = tfidf_vect.get_feature_names()\n",
        "terms1[1:4]\n",
        "df1 = df_1\n",
        "df1['Tfidf Clus Label'] = model_tf.labels_\n",
        "df1.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24RUrSOkXmZr"
      },
      "source": [
        "df1.groupby(['Tfidf Clus Label'])['cleaned_text'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHpiHLtmXpVa"
      },
      "source": [
        "print(\"Highest terms per cluster:\")\n",
        "order_centroids = model_tf.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(1,4):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :4]:\n",
        "        print(' %s' % terms1[ind], end='')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtzPF5ByXyJN"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXL9GXoUX1Vz"
      },
      "source": [
        "i=0\n",
        "list_of_sent=[]\n",
        "for sent in df_1['cleaned_text'].values:\n",
        "    list_of_sent.append(sent.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kt6sxnAX424"
      },
      "source": [
        "w2v_model=gensim.models.Word2Vec(list_of_sent,size=90, workers=4)\n",
        "import numpy as np\n",
        "sent_vectors = [];\n",
        "for sent in list_of_sent: \n",
        "    sent_vec = np.zeros(90) \n",
        "    cnt_words =0; \n",
        "    for word in sent: \n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /= cnt_words\n",
        "    sent_vectors.append(sent_vec)\n",
        "sent_vectors = np.array(sent_vectors)\n",
        "sent_vectors = np.nan_to_num(sent_vectors)\n",
        "sent_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUjKUl3eYIJN"
      },
      "source": [
        "minPts = 2 * 100\n",
        "def lower_bound(nums, target): \n",
        "    l, r = 0, len(nums) - 1\n",
        "    while l <= r: \n",
        "        mid = int(l + (r - l) / 2)\n",
        "        if nums[mid] >= target:\n",
        "            r = mid - 1\n",
        "        else:\n",
        "            l = mid + 1\n",
        "    return l\n",
        "\n",
        "def compute200thnearestneighbour(a, data): \n",
        "    dists = []\n",
        "    for val in data:\n",
        "        dist = np.sum((a - val) **2 ) \n",
        "        if(len(dists) == 200 and dists[199] > dist): \n",
        "            l = int(lower_bound(dists, dist)) \n",
        "            if l < 200 and l >= 0 and dists[l] > dist:\n",
        "                dists[l] = dist\n",
        "        else:\n",
        "            dists.append(dist)\n",
        "            dists.sort()\n",
        "    \n",
        "    return dists[199]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMfbjscPYgB1"
      },
      "source": [
        "twohundrethneigh = []\n",
        "for val in sent_vectors[:1000]:\n",
        "    twohundrethneigh.append( compute200thnearestneighbour(val, sent_vectors[:1000]) )\n",
        "twohundrethneigh.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruz36GI9YmrH"
      },
      "source": [
        "mob_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp0YK9PpYtbT"
      },
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "plt.title(\"Finding the correct Eps hyperparameter by using elbow method\")\n",
        "plt.plot([a for a in range(len(twohundrethneigh))], twohundrethneigh)\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Distance of 200th Nearest Neighbour\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNKnXSvGYtiC"
      },
      "source": [
        "model = DBSCAN(eps = 4, min_samples = minPts, n_jobs=-1)\n",
        "model.fit(sent_vectors)\n",
        "df_1['AVG-W2V Clus Label'] = model.labels_\n",
        "df_1.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlPM59U6ZB9j"
      },
      "source": [
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "dendro=hierarchy.dendrogram(hierarchy.linkage(sent_vectors,method='ward'))\n",
        "plt.axhline(y=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St5EGI8KZGTM"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')  \n",
        "Agg=cluster.fit_predict(sent_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1pchwCCZZJd"
      },
      "source": [
        "df_1['AVG-W2V Clus Label'] = cluster.labels_\n",
        "df_1.head(3)\n",
        "df_1.groupby(['AVG-W2V Clus Label'])['cleaned_text'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfiRq6NSZcPY"
      },
      "source": [
        "for i in range(4):\n",
        "    print(\"3 reviews of given cluster \", i)\n",
        "    print(\"-\" * 70)\n",
        "    print(df_1.iloc[df_1.groupby(['AVG-W2V Clus Label']).groups[i][0]]['cleaned_text'])\n",
        "    print('\\n')\n",
        "    print(df_1.iloc[df_1.groupby(['AVG-W2V Clus Label']).groups[i][1]]['cleaned_text'])\n",
        "    print('\\n')\n",
        "    print(\"_\" * 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rWrUMfZZlDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUaOTvSnTC25"
      },
      "source": [
        "#You can write you answer here.\n",
        "Cluster analysis is the task of grouping a set of objects in such a way that objects in the same group. \n",
        "It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, \n",
        "including pattern recognition, image analysis and machine learning. The K-means clustering algorithm is used to find groups which have not been \n",
        "explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or\n",
        " to identify unknown groups in complex data sets. To use k means clustering the first step we need to do is select the number of clusters \n",
        " Second, Select k random points from the data as centroids and then Assign all the points to the closest cluster centroid.\n",
        "Recompute the centroids of newly formed clusters and then repeat the third and 4th steps untill the requirement is met\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}