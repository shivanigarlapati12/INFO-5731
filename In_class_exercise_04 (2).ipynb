{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR0L3_CreM_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e81caaf-c66d-40a9-c96a-2af4553bf41c"
      },
      "source": [
        "# Write your code here\n",
        "#pip install nltk\n",
        "#1.1\n",
        "import glob\n",
        "import re\n",
        "import csv\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords.words('english')\n",
        "S_count = 0\n",
        "W_count = 0\n",
        "Ch_count = 0\n",
        "wl_count=0\n",
        "spec_count=0\n",
        "num =0\n",
        "stopWordCount =0\n",
        "upperCase_count =0\n",
        "file1 = open('01-05-1  Adams v Tanner.txt', 'r') \n",
        "Lines = file1.readlines() \n",
        "stopWords=stopwords.words('english')\n",
        "for line in Lines:\n",
        "  count = count + 1\n",
        "  Ch_count = Ch_count + len(line)\n",
        "  sent = line.split(\".\")\n",
        "  for sentence in sent:\n",
        "    if len(sentence)>0:\n",
        "      filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
        "      words_without_punc = [word for word in filtered.split() if word]\n",
        "      wl_count = wl_count + sum(map(len, words_without_punc))\n",
        "      words=sentence.split(\" \")\n",
        "      for eachWord in words:\n",
        "        new = re.sub('[\\w]+' ,'', eachWord)\n",
        "        spec_count = spec_count + len(new)\n",
        "        if eachWord.isupper()==True:\n",
        "          upperCase_count=upperCase_count+1\n",
        "        if len(eachWord) > 0:\n",
        "          if eachWord.isalnum() == True:\n",
        "            W_count = W_count + 1\n",
        "          if eachWord in stopWords:\n",
        "            stopWordCount = stopWordCount + 1\n",
        "          if eachWord[0].isnumeric()==True:\n",
        "            num = num+1\n",
        "      S_count = S_count +1\n",
        "print(\"count of sentences=\", S_count)\n",
        "print(\"count of words=\", W_count)\n",
        "print(\"count of characters=\", Ch_count)\n",
        "print(\"average length of words is=\", wl_count/W_count)\n",
        "print(\"count of special characters is=\", spec_count)\n",
        "print(\"number of numerics=\", num)\n",
        "print(\"stop word count =\", stopWordCount)\n",
        "print(\"count of upper case words =\", upperCase_count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#1.2\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stopWords=stopwords.words('english')\n",
        "givenText= open('01-05-1  Adams v Tanner.txt', 'r').read()\n",
        "sentences=nltk.tokenize.sent_tokenize(givenText)\n",
        "tokenedData={'statements':sentences}\n",
        "dataFrame=pd.DataFrame(data=tokenedData)\n",
        "\n",
        "dataFrame['statements'] = dataFrame['statements'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "print(\"1)post lower casing: \",dataFrame['statements'].head())\n",
        "\n",
        "dataFrame['statements'] = dataFrame['statements'].str.replace('[^\\w\\s]','')\n",
        "print(\"2)after removing punctuaions\",dataFrame['statements'].head())\n",
        "\n",
        "dataFrame['statements'] = dataFrame['statements'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopWords))\n",
        "print(\"3)after removal of stop words:\",dataFrame['statements'].head())\n",
        "\n",
        "Frequent = pd.Series(' '.join(dataFrame['statements']).split()).value_counts()[:10]\n",
        "Frequent=list(Frequent.index)\n",
        "dataFrame['statements'] = dataFrame['statements'].apply(lambda x: \" \".join(x for x in x.split() if x not in Frequent))\n",
        "print(\"4)frequent words removal \",dataFrame['statements'].head())\n",
        "\n",
        "rare = pd.Series(' '.join(dataFrame['statements']).split()).value_counts()[-10:]\n",
        "rare=list(rare.index)\n",
        "dataFrame['statements'] = dataFrame['statements'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
        "print(\"5)rare words removal\",dataFrame['statements'].head())\n",
        "\n",
        "dataFrame['statements'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
        "print(\"6)post spelling correction\",dataFrame['statements'].head())\n",
        "\n",
        "print(\"7) tokenization\", TextBlob(dataFrame['statements'][1]).words)\n",
        "\n",
        "stem = PorterStemmer()\n",
        "print(\"8)stemming\",dataFrame['statements'][:5].apply(lambda x: \" \".join([stem.stem(word) for word in x.split()])))\n",
        "\n",
        "dataFrame['statements'] = dataFrame['statements'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "print(\"9)lemmeatiaztion:\",dataFrame['statements'].head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#1.3\n",
        "dataFrame.to_csv('clean.csv',index=False)\n",
        "\n",
        "\n",
        "\n",
        "#1.4\n",
        "frequency = pd.Series(' '.join(dataFrame['statements']).split()).value_counts()\n",
        "print(\"frequency for each term \",frequency)\n",
        "\n",
        "TextBlob(dataFrame['statements'][0]).ngrams(1)\n",
        "TextBlob(dataFrame['statements'][0]).ngrams(2)\n",
        "TextBlob(dataFrame['statements'][0]).ngrams(3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "count of sentences= 442\n",
            "count of words= 3167\n",
            "count of characters= 20454\n",
            "average length of words is= 5.061256709820019\n",
            "count of special characters is= 686\n",
            "number of numerics= 134\n",
            "stop word count = 1676\n",
            "count of upper case words = 92\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "1)post lower casing:  0                 5 ala. 740 supreme court of alabama.\n",
            "1                          adams v. tanner and horton.\n",
            "2                                     june term, 1843.\n",
            "3    synopsis writ of error to the circuit court of...\n",
            "4    west headnotes (2) [1] chattel mortgages crops...\n",
            "Name: statements, dtype: object\n",
            "2)after removing punctuaions 0                   5 ala 740 supreme court of alabama\n",
            "1                            adams v tanner and horton\n",
            "2                                       june term 1843\n",
            "3    synopsis writ of error to the circuit court of...\n",
            "4    west headnotes 2 1 chattel mortgages crops a g...\n",
            "Name: statements, dtype: object\n",
            "3)after removal of stop words: 0                      5 ala 740 supreme court alabama\n",
            "1                                adams v tanner horton\n",
            "2                                       june term 1843\n",
            "3             synopsis writ error circuit court sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: statements, dtype: object\n",
            "4)frequent words removal  0                            5 ala 740 supreme alabama\n",
            "1                                  adams tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: statements, dtype: object\n",
            "5)rare words removal 0                            5 ala 740 supreme alabama\n",
            "1                                  adams tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: statements, dtype: object\n",
            "6)post spelling correction 0                            5 ala 740 supreme alabama\n",
            "1                                  adams tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: statements, dtype: object\n",
            "7) tokenization ['adams', 'tanner', 'horton']\n",
            "8)stemming 0                             5 ala 740 suprem alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                    synopsi writ error circuit sumter\n",
            "4    west headnot 2 1 chattel mortgag crop grow exi...\n",
            "Name: statements, dtype: object\n",
            "9)lemmeatiaztion: 0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: statements, dtype: object\n",
            "frequency for each term  rep              16\n",
            "contract         16\n",
            "growing          16\n",
            "law              15\n",
            "plaintiff        15\n",
            "                 ..\n",
            "741               1\n",
            "reuters           1\n",
            "retrospective     1\n",
            "approved          1\n",
            "involves          1\n",
            "Length: 782, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['5', 'ala', '740']),\n",
              " WordList(['ala', '740', 'supreme']),\n",
              " WordList(['740', 'supreme', 'alabama'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wSv6fVhOfFmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc7037a5-dc97-4104-8de3-0d875303c4fe"
      },
      "source": [
        "# Write your code here\n",
        "ip = \"260.08.094.109\"\n",
        "result = \"\"\n",
        "nums = ip.split(\".\")\n",
        "for num in nums:\n",
        "  if num[0]=='0':\n",
        "    num = num[1:]\n",
        "  result = result + num + \".\"\n",
        "result = result[0:len(result)-1]\n",
        "print(result)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "756df422-8814-4946-a108-79c6d1b3bbb2"
      },
      "source": [
        "# Write your code here\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "words = sentence.split(\" \")\n",
        "for word in words:\n",
        "  if word[0]=='2':\n",
        "    s = \"\";\n",
        "    for letter in word:\n",
        "      if letter.isnumeric() == True:\n",
        "        s = s + letter\n",
        "    print(s)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2010\n",
            "2010\n",
            "2019\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}